{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd1151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import torch\n",
    "import os \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "labels = []\n",
    "audio_files = [\"Audio_Speech_Actors_01-24\", \"Audio_Song_Actors_01-24\"]\n",
    "drive_base_path = \"/content/drive/MyDrive/MARS\"\n",
    "\n",
    "for main_file in audio_files:\n",
    "    main_file_path = os.path.join(drive_base_path, main_file)\n",
    "    for actor in os.listdir(main_file_path):\n",
    "        actor_path = os.path.join(main_file_path, actor)\n",
    "        for audio in os.listdir(actor_path):\n",
    "            if audio.endswith(\".wav\"):\n",
    "                path = os.path.join(actor_path, audio)\n",
    "                paths.append(path)\n",
    "                emotion = int(audio.split(\"-\")[2])\n",
    "                labels.append(emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a69861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of paths:  2452\n",
      "Length of labels:  2452\n",
      "Maximum label:  8\n",
      "Minimum label:  1\n",
      "Data type of paths:  <class 'str'>\n",
      "Data type of lables:  <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of paths: \",len(paths))\n",
    "print(\"Length of labels: \",len(labels))\n",
    "print(\"Maximum label: \",max(labels))\n",
    "print(\"Minimum label: \",min(labels))\n",
    "print(\"Data type of paths: \",type(paths[0]))\n",
    "print(\"Data type of lables: \",type(labels[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ae220e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Audio_Song_Actors_01-24\\\\Actor_24\\\\03-02-03-02-02-01-24.wav', 3),\n",
       " ('Audio_Song_Actors_01-24\\\\Actor_02\\\\03-02-03-02-02-02-02.wav', 3),\n",
       " ('Audio_Speech_Actors_01-24\\\\Actor_14\\\\03-01-03-01-02-01-14.wav', 3),\n",
       " ('Audio_Speech_Actors_01-24\\\\Actor_17\\\\03-01-07-01-02-01-17.wav', 7),\n",
       " ('Audio_Song_Actors_01-24\\\\Actor_06\\\\03-02-06-01-02-01-06.wav', 6)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(zip(paths, labels))\n",
    "seed = random.Random(42)\n",
    "seed.shuffle(data)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d899006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audio_Song_Actors_01-24\\Actor_24\\03-02-03-02-0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audio_Song_Actors_01-24\\Actor_02\\03-02-03-02-0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Audio_Speech_Actors_01-24\\Actor_14\\03-01-03-01...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audio_Speech_Actors_01-24\\Actor_17\\03-01-07-01...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio_Song_Actors_01-24\\Actor_06\\03-02-06-01-0...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  label\n",
       "0  Audio_Song_Actors_01-24\\Actor_24\\03-02-03-02-0...      3\n",
       "1  Audio_Song_Actors_01-24\\Actor_02\\03-02-03-02-0...      3\n",
       "2  Audio_Speech_Actors_01-24\\Actor_14\\03-01-03-01...      3\n",
       "3  Audio_Speech_Actors_01-24\\Actor_17\\03-01-07-01...      7\n",
       "4  Audio_Song_Actors_01-24\\Actor_06\\03-02-06-01-0...      6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.DataFrame(data, columns=[\"path\",\"label\"])\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66bf93d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2452 entries, 0 to 2451\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   path    2452 non-null   object\n",
      " 1   label   2452 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 38.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c94b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.318108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.020284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "count  2452.000000\n",
       "mean      4.318108\n",
       "std       2.020284\n",
       "min       1.000000\n",
       "25%       3.000000\n",
       "50%       4.000000\n",
       "75%       6.000000\n",
       "max       8.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57157e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path     Audio_Song_Actors_01-24\\Actor_24\\03-02-03-02-0...\n",
       "label                                                    3\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecc2e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "df_combined[\"path\"] = df_combined[\"path\"].str.replace(\"\\\\\", \"/\", regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53dcf9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_24/03-02-03-02-0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_02/03-02-03-02-0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_14/03-01-03-01...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_17/03-01-07-01...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_06/03-02-06-01-0...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  label\n",
       "0  Audio_Song_Actors_01-24/Actor_24/03-02-03-02-0...      3\n",
       "1  Audio_Song_Actors_01-24/Actor_02/03-02-03-02-0...      3\n",
       "2  Audio_Speech_Actors_01-24/Actor_14/03-01-03-01...      3\n",
       "3  Audio_Speech_Actors_01-24/Actor_17/03-01-07-01...      7\n",
       "4  Audio_Song_Actors_01-24/Actor_06/03-02-06-01-0...      6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e465b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum label value:  0\n",
      "maximum label value:  7\n"
     ]
    }
   ],
   "source": [
    "df_combined[\"label\"] = df_combined[\"label\"].apply(lambda x: x-1)\n",
    "print(\"minimum label value: \", df_combined[\"label\"].min())\n",
    "print(\"maximum label value: \", df_combined[\"label\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b8d4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data = train_test_split(df_combined, test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "218ea4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_08/03-01-08-01...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_05/03-01-01-01...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_15/03-02-03-02-0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_05/03-01-08-02...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_20/03-01-02-02...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  label\n",
       "2138  Audio_Speech_Actors_01-24/Actor_08/03-01-08-01...      7\n",
       "623   Audio_Speech_Actors_01-24/Actor_05/03-01-01-01...      0\n",
       "1942  Audio_Song_Actors_01-24/Actor_15/03-02-03-02-0...      2\n",
       "1331  Audio_Speech_Actors_01-24/Actor_05/03-01-08-02...      7\n",
       "401   Audio_Speech_Actors_01-24/Actor_20/03-01-02-02...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "141f951c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_16/03-01-08-02...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_20/03-01-08-02...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_17/03-02-05-02-0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2267</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_24/03-02-04-01-0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_13/03-01-04-02...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  label\n",
       "1794  Audio_Speech_Actors_01-24/Actor_16/03-01-08-02...      7\n",
       "1833  Audio_Speech_Actors_01-24/Actor_20/03-01-08-02...      7\n",
       "1488  Audio_Song_Actors_01-24/Actor_17/03-02-05-02-0...      4\n",
       "2267  Audio_Song_Actors_01-24/Actor_24/03-02-04-01-0...      3\n",
       "290   Audio_Speech_Actors_01-24/Actor_13/03-01-04-02...      3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b839998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training Data:  1961\n",
      "Length of Validation Data:  491\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Training Data: \", len(training_data))\n",
    "print(\"Length of Validation Data: \", len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b88d1697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237037,) 48000\n",
      "(79013,) 16000\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf \n",
    "import torchaudio\n",
    "import librosa\n",
    "waveform, sr = sf.read(training_data.loc[0,\"path\"])\n",
    "print((waveform.shape),sr)\n",
    "waveform, sr = librosa.load(training_data.loc[0,\"path\"], sr=16000)\n",
    "print(waveform.shape, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "059c0a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max duration: 6.37306250 seconds\n",
      "sample rate: 16000.00\n",
      "Longest file: Audio_Song_Actors_01-24/Actor_22/03-02-02-02-02-01-22.wav\n"
     ]
    }
   ],
   "source": [
    "max_duration = 0\n",
    "longest_file = \"\"\n",
    "for path in df_combined['path']:\n",
    "    # full_path = os.path.join(base_dir, path)\n",
    "    if os.path.isfile(path):\n",
    "        waveform, sample_rate = librosa.load(path, sr=16000)\n",
    "        duration = waveform.shape[0] / sample_rate\n",
    "        if duration > max_duration:\n",
    "            max_duration = duration\n",
    "            longest_file = path\n",
    "    else:\n",
    "        print(f\"one file path is not available {path}\")\n",
    "        break\n",
    "\n",
    "print(f\"Max duration: {max_duration:.8f} seconds\")\n",
    "print(f\"sample rate: {sample_rate:.2f}\")\n",
    "print(f\"Longest file: {longest_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bd6ea75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101969,) 16000\n"
     ]
    }
   ],
   "source": [
    "waveform, sr = librosa.load(\"Audio_Song_Actors_01-24/Actor_22/03-02-02-02-02-01-22.wav\", sr =16000)\n",
    "print(waveform.shape, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "244a9a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({16000: 2452})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "sample_rates = []\n",
    "for full_path in df_combined['path']:\n",
    "    # full_path = os.path.join(base_dir, path)\n",
    "    if os.path.isfile(full_path):\n",
    "        _, sr = librosa.load(full_path, sr=16000)\n",
    "        sample_rates.append(sr)\n",
    "\n",
    "# Count frequency of each sample rate\n",
    "rate_counts = Counter(sample_rates)\n",
    "print(rate_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1db898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data, processor, max_length= int(6.37306250*16000)):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        audio_path = self.data.iloc[index][\"path\"]\n",
    "        label = self.data.iloc[index][\"label\"]\n",
    "\n",
    "        audio, sr = librosa.load(audio_path)\n",
    "        audio = audio.squeeze()\n",
    "\n",
    "        if len(audio) > self.max_length :\n",
    "            audio = audio[:self.max_length]\n",
    "            # print(f\"found a audio file greater than max length : {audio_path} with an audio length {len(audio)}\")\n",
    "        else:\n",
    "            audio = np.pad(audio, (0,int(self.max_length-len(audio))), \"constant\")\n",
    "            \n",
    "        inputs = self.processor(audio, sampling_rate=16000, return_tensors='pt', padding=True, truncate=True, max_length=self.max_length)\n",
    "        input_values = inputs.input_values.squeeze()\n",
    "\n",
    "        return {'input_values': input_values, 'labels': torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93642dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6497061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akhilesh\\.conda\\envs\\akhilesh-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\akhilesh\\.conda\\envs\\akhilesh-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\akhilesh\\.conda\\envs\\akhilesh-env\\Lib\\site-packages\\transformers\\configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, Wav2Vec2Config, Trainer, TrainingArguments\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-large\", num_labels=8)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96b2ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(training_data, processor)\n",
    "validation_dataset = AudioDataset(validation_data, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a77c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dd7994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ec178cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.39.3\n"
     ]
    }
   ],
   "source": [
    "from transformers import __version__\n",
    "print(__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f876c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-large\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    save_total_limit=10,\n",
    "    logging_dir=\"./wav2vec2-large/logs\",\n",
    "    greater_is_better=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15ce0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def get_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    return {\n",
    "        'accuracy':accuracy,\n",
    "        'precision':precision,\n",
    "        'recall':recall,\n",
    "        'fscore':fscore\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdecbd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c42e9809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akhilesh\\.conda\\envs\\akhilesh-env\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=get_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9d96935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1,961\n",
      "  Num Epochs = 19\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 2,337\n",
      "  Number of trainable parameters = 315,693,448\n",
      "  5%|▌         | 123/2337 [06:21<1:44:20,  2.83s/it]***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9064, 'grad_norm': 11.772726058959961, 'learning_rate': 1.894736842105263e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akhilesh\\.conda\\envs\\akhilesh-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "                                                    \n",
      "  5%|▌         | 123/2337 [06:57<1:44:20,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6625241041183472, 'eval_accuracy': 0.3727087576374745, 'eval_precision': 0.3084582023706751, 'eval_recall': 0.3727087576374745, 'eval_fscore': 0.2881233079036416, 'eval_runtime': 35.7512, 'eval_samples_per_second': 13.734, 'eval_steps_per_second': 1.734, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./wav2vec2-large\\checkpoint-123\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-123\\config.json\n",
      "Model weights saved in ./wav2vec2-large\\checkpoint-123\\model.safetensors\n",
      "Deleting older checkpoint [wav2vec2-large\\checkpoint-246] due to args.save_total_limit\n",
      " 11%|█         | 246/2337 [13:46<1:37:07,  2.79s/it] ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5752, 'grad_norm': 35.67006301879883, 'learning_rate': 1.7894736842105264e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 11%|█         | 246/2337 [14:22<1:37:07,  2.79s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-246\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-246\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4160149097442627, 'eval_accuracy': 0.4419551934826884, 'eval_precision': 0.48137021794493967, 'eval_recall': 0.4419551934826884, 'eval_fscore': 0.3841170165094852, 'eval_runtime': 35.8858, 'eval_samples_per_second': 13.682, 'eval_steps_per_second': 1.728, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-246\\model.safetensors\n",
      "Deleting older checkpoint [wav2vec2-large\\checkpoint-123] due to args.save_total_limit\n",
      " 16%|█▌        | 369/2337 [20:43<1:29:09,  2.72s/it]***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2409, 'grad_norm': 4.1494903564453125, 'learning_rate': 1.6842105263157896e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 16%|█▌        | 369/2337 [21:17<1:29:09,  2.72s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-369\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-369\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1935038566589355, 'eval_accuracy': 0.5926680244399185, 'eval_precision': 0.6305708950838942, 'eval_recall': 0.5926680244399185, 'eval_fscore': 0.5622118872112042, 'eval_runtime': 34.3578, 'eval_samples_per_second': 14.291, 'eval_steps_per_second': 1.805, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-369\\model.safetensors\n",
      "Deleting older checkpoint [wav2vec2-large\\checkpoint-246] due to args.save_total_limit\n",
      " 21%|██        | 492/2337 [27:42<1:24:25,  2.75s/it]***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8977, 'grad_norm': 6.837130069732666, 'learning_rate': 1.578947368421053e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 21%|██        | 492/2337 [28:17<1:24:25,  2.75s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-492\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-492\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9394488334655762, 'eval_accuracy': 0.6924643584521385, 'eval_precision': 0.7182692366817568, 'eval_recall': 0.6924643584521385, 'eval_fscore': 0.6748167575183395, 'eval_runtime': 34.3736, 'eval_samples_per_second': 14.284, 'eval_steps_per_second': 1.804, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-492\\model.safetensors\n",
      "Deleting older checkpoint [wav2vec2-large\\checkpoint-369] due to args.save_total_limit\n",
      " 26%|██▋       | 615/2337 [34:56<1:18:54,  2.75s/it] ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7507, 'grad_norm': 11.67033863067627, 'learning_rate': 1.4736842105263159e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 26%|██▋       | 615/2337 [35:31<1:18:54,  2.75s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-615\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-615\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8867451548576355, 'eval_accuracy': 0.7067209775967414, 'eval_precision': 0.7295698366343304, 'eval_recall': 0.7067209775967414, 'eval_fscore': 0.6941280559043705, 'eval_runtime': 34.5509, 'eval_samples_per_second': 14.211, 'eval_steps_per_second': 1.794, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-615\\model.safetensors\n",
      "Deleting older checkpoint [wav2vec2-large\\checkpoint-492] due to args.save_total_limit\n",
      " 32%|███▏      | 738/2337 [42:22<1:12:01,  2.70s/it] ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5938, 'grad_norm': 74.11929321289062, 'learning_rate': 1.3684210526315791e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 32%|███▏      | 738/2337 [42:58<1:12:01,  2.70s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-738\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-738\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7368270754814148, 'eval_accuracy': 0.7718940936863544, 'eval_precision': 0.7879950134429433, 'eval_recall': 0.7718940936863544, 'eval_fscore': 0.7694035363482191, 'eval_runtime': 35.837, 'eval_samples_per_second': 13.701, 'eval_steps_per_second': 1.73, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-738\\model.safetensors\n",
      " 37%|███▋      | 861/2337 [56:43<3:57:39,  9.66s/it]***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4914, 'grad_norm': 13.611947059631348, 'learning_rate': 1.263157894736842e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 37%|███▋      | 861/2337 [58:38<3:57:39,  9.66s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-861\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-861\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.715597927570343, 'eval_accuracy': 0.7922606924643585, 'eval_precision': 0.8097401585556814, 'eval_recall': 0.7922606924643585, 'eval_fscore': 0.790948071056942, 'eval_runtime': 115.4483, 'eval_samples_per_second': 4.253, 'eval_steps_per_second': 0.537, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-861\\model.safetensors\n",
      " 42%|████▏     | 984/2337 [1:20:53<3:33:55,  9.49s/it]***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4096, 'grad_norm': 77.42636108398438, 'learning_rate': 1.1578947368421053e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 42%|████▏     | 984/2337 [1:22:49<3:33:55,  9.49s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-984\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-984\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9186778664588928, 'eval_accuracy': 0.7617107942973523, 'eval_precision': 0.797199935668136, 'eval_recall': 0.7617107942973523, 'eval_fscore': 0.7558129068728657, 'eval_runtime': 116.2041, 'eval_samples_per_second': 4.225, 'eval_steps_per_second': 0.534, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-984\\model.safetensors\n",
      " 47%|████▋     | 1107/2337 [1:45:10<3:14:55,  9.51s/it]***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3456, 'grad_norm': 31.8961124420166, 'learning_rate': 1.0526315789473684e-05, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 47%|████▋     | 1107/2337 [1:47:06<3:14:55,  9.51s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-1107\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-1107\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8136625289916992, 'eval_accuracy': 0.769857433808554, 'eval_precision': 0.8199336937348892, 'eval_recall': 0.769857433808554, 'eval_fscore': 0.7683072065498567, 'eval_runtime': 115.8418, 'eval_samples_per_second': 4.239, 'eval_steps_per_second': 0.535, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-1107\\model.safetensors\n",
      " 53%|█████▎    | 1230/2337 [2:09:35<2:56:35,  9.57s/it] ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2876, 'grad_norm': 5.187353134155273, 'learning_rate': 9.473684210526315e-06, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 53%|█████▎    | 1230/2337 [2:11:31<2:56:35,  9.57s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-1230\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-1230\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7763690948486328, 'eval_accuracy': 0.8167006109979633, 'eval_precision': 0.8330097202245799, 'eval_recall': 0.8167006109979633, 'eval_fscore': 0.8137089104170204, 'eval_runtime': 116.5439, 'eval_samples_per_second': 4.213, 'eval_steps_per_second': 0.532, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-1230\\model.safetensors\n",
      "Deleting older checkpoint [wav2vec2-large\\checkpoint-615] due to args.save_total_limit\n",
      " 58%|█████▊    | 1353/2337 [2:27:31<43:09,  2.63s/it]   ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1913, 'grad_norm': 10.44920825958252, 'learning_rate': 8.421052631578948e-06, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 58%|█████▊    | 1353/2337 [2:28:05<43:09,  2.63s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-1353\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-1353\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8816369771957397, 'eval_accuracy': 0.814663951120163, 'eval_precision': 0.8434726944324458, 'eval_recall': 0.814663951120163, 'eval_fscore': 0.8153558914137363, 'eval_runtime': 34.0534, 'eval_samples_per_second': 14.419, 'eval_steps_per_second': 1.821, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-1353\\model.safetensors\n",
      " 63%|██████▎   | 1476/2337 [2:34:39<39:06,  2.73s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1607, 'grad_norm': 112.14482879638672, 'learning_rate': 7.368421052631579e-06, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 63%|██████▎   | 1476/2337 [2:35:13<39:06,  2.73s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-1476\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-1476\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8583009839057922, 'eval_accuracy': 0.8329938900203666, 'eval_precision': 0.856434827834732, 'eval_recall': 0.8329938900203666, 'eval_fscore': 0.8337764201791169, 'eval_runtime': 34.4322, 'eval_samples_per_second': 14.26, 'eval_steps_per_second': 1.801, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-1476\\model.safetensors\n",
      " 68%|██████▊   | 1599/2337 [2:41:34<33:04,  2.69s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1356, 'grad_norm': 31.450481414794922, 'learning_rate': 6.31578947368421e-06, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 68%|██████▊   | 1599/2337 [2:42:09<33:04,  2.69s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-1599\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-1599\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7978490591049194, 'eval_accuracy': 0.845213849287169, 'eval_precision': 0.8552578781502548, 'eval_recall': 0.845213849287169, 'eval_fscore': 0.842188796117147, 'eval_runtime': 34.6136, 'eval_samples_per_second': 14.185, 'eval_steps_per_second': 1.791, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-1599\\model.safetensors\n",
      "Deleting older checkpoint [wav2vec2-large\\checkpoint-738] due to args.save_total_limit\n",
      " 74%|███████▎  | 1722/2337 [2:48:42<27:46,  2.71s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1092, 'grad_norm': 38.83863067626953, 'learning_rate': 5.263157894736842e-06, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 74%|███████▎  | 1722/2337 [2:49:17<27:46,  2.71s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-1722\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-1722\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7833456993103027, 'eval_accuracy': 0.8553971486761711, 'eval_precision': 0.8704182072304655, 'eval_recall': 0.8553971486761711, 'eval_fscore': 0.8562505351000519, 'eval_runtime': 34.4751, 'eval_samples_per_second': 14.242, 'eval_steps_per_second': 1.798, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-1722\\model.safetensors\n",
      "Deleting older checkpoint [wav2vec2-large\\checkpoint-861] due to args.save_total_limit\n",
      " 79%|███████▉  | 1845/2337 [2:55:40<22:40,  2.77s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0776, 'grad_norm': 0.08204229921102524, 'learning_rate': 4.210526315789474e-06, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 79%|███████▉  | 1845/2337 [2:56:14<22:40,  2.77s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-1845\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-1845\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8844490647315979, 'eval_accuracy': 0.8492871690427699, 'eval_precision': 0.8617661720537513, 'eval_recall': 0.8492871690427699, 'eval_fscore': 0.8496078550168151, 'eval_runtime': 34.4589, 'eval_samples_per_second': 14.249, 'eval_steps_per_second': 1.799, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-1845\\model.safetensors\n",
      "Deleting older checkpoint [wav2vec2-large\\checkpoint-984] due to args.save_total_limit\n",
      " 84%|████████▍ | 1968/2337 [3:02:53<16:31,  2.69s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0572, 'grad_norm': 6.022532939910889, 'learning_rate': 3.157894736842105e-06, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 84%|████████▍ | 1968/2337 [3:03:27<16:31,  2.69s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-1968\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-1968\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7746022939682007, 'eval_accuracy': 0.8615071283095723, 'eval_precision': 0.8708490430421506, 'eval_recall': 0.8615071283095723, 'eval_fscore': 0.860070991088992, 'eval_runtime': 34.4619, 'eval_samples_per_second': 14.248, 'eval_steps_per_second': 1.799, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-1968\\model.safetensors\n",
      "Deleting older checkpoint [wav2vec2-large\\checkpoint-1107] due to args.save_total_limit\n",
      " 89%|████████▉ | 2091/2337 [3:09:48<11:08,  2.72s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.059, 'grad_norm': 0.11070801317691803, 'learning_rate': 2.105263157894737e-06, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 89%|████████▉ | 2091/2337 [3:10:22<11:08,  2.72s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-2091\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-2091\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8993044495582581, 'eval_accuracy': 0.8513238289205702, 'eval_precision': 0.860713740709382, 'eval_recall': 0.8513238289205702, 'eval_fscore': 0.8492581510565718, 'eval_runtime': 34.4322, 'eval_samples_per_second': 14.26, 'eval_steps_per_second': 1.801, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-2091\\model.safetensors\n",
      " 95%|█████████▍| 2214/2337 [3:17:02<05:33,  2.71s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0321, 'grad_norm': 0.09706176817417145, 'learning_rate': 1.0526315789473685e-06, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 95%|█████████▍| 2214/2337 [3:17:36<05:33,  2.71s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-2214\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-2214\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7754173278808594, 'eval_accuracy': 0.8757637474541752, 'eval_precision': 0.8845444727995149, 'eval_recall': 0.8757637474541752, 'eval_fscore': 0.8753336214574016, 'eval_runtime': 34.4781, 'eval_samples_per_second': 14.241, 'eval_steps_per_second': 1.798, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-2214\\model.safetensors\n",
      "100%|██████████| 2337/2337 [3:24:08<00:00,  2.67s/it]***** Running Evaluation *****\n",
      "  Num examples = 491\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.039, 'grad_norm': 1.7784452438354492, 'learning_rate': 0.0, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 2337/2337 [3:24:43<00:00,  2.67s/it]Saving model checkpoint to ./wav2vec2-large\\checkpoint-2337\n",
      "Configuration saved in ./wav2vec2-large\\checkpoint-2337\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7227576375007629, 'eval_accuracy': 0.8839103869653768, 'eval_precision': 0.8871974205525632, 'eval_recall': 0.8839103869653768, 'eval_fscore': 0.881463162315191, 'eval_runtime': 34.477, 'eval_samples_per_second': 14.241, 'eval_steps_per_second': 1.798, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./wav2vec2-large\\checkpoint-2337\\model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 2337/2337 [3:24:55<00:00,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 12295.9256, 'train_samples_per_second': 3.03, 'train_steps_per_second': 0.19, 'train_loss': 0.4926736000821689, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2337, training_loss=0.4926736000821689, metrics={'train_runtime': 12295.9256, 'train_samples_per_second': 3.03, 'train_steps_per_second': 0.19, 'train_loss': 0.4926736000821689, 'epoch': 19.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96551e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=\"Give the path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde07cd",
   "metadata": {},
   "source": [
    "means that you are instructing the Hugging Face Trainer to resume training from a previously saved checkpoint located at the specified path (\"Give the path\"). This allows you to continue training from where you left off, rather than starting over from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a062f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3251c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "akhilesh-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
