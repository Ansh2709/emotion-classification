# emotion-classification
Based on Machine Learning and Deep Learning

Emotion Classification from Speech - README
Project Overview
This repository provides a comprehensive pipeline for speech emotion classification using deep learning and ensemble machine learning models. The system processes raw audio files, extracts relevant acoustic features, and classifies them into one of several emotional states. The solution includes:

Jupyter notebooks for model training (DNN, XGBoost, etc.)

A Python script for model testing on new audio files

A Streamlit web application for interactive emotion prediction

All necessary model artifacts and configuration files

![Project Flow](https://github.com/user-attachments/assets/edd713f2-e662-43df-9c7e-c0f87ec96f7f)




Directory Structure




![Screenshot 2025-06-25 023125](https://github.com/user-attachments/assets/2fd8bd26-7843-4b4c-a997-73649a352dd6)


# ğŸ¤ Speech Emotion Recognition System

> A sophisticated machine learning system for classifying emotions from speech signals with multiple model architectures and robust performance across diverse emotional classes.

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://tensorflow.org/)

---

## ğŸŒŸ Features

- **Multi-Architecture Support**: DNN, XGBoost, Random Forest, and ensemble methods
- **Advanced Feature Extraction**: MFCCs, Chroma, Mel Spectrogram, and spectral features
- **Real-time Processing**: Command-line interface and interactive web application
- **Robust Performance**: Optimized for diverse emotional classes with data augmentation
- **Production Ready**: Complete pipeline from preprocessing to deployment

---

## ğŸ—ï¸ System Architecture

```
ğŸ“ Raw Audio Input
    â†“
ğŸ”§ Audio Preprocessing (22.05kHz, 3s duration)
    â†“
ğŸ¯ Feature Extraction (MFCCs, Chroma, Spectral)
    â†“
âš¡ Model Training (DNN/XGBoost/RF)
    â†“
ğŸš€ Deployment (CLI/Web Interface)
```

---

## ğŸ”¬ Preprocessing Pipeline

### Audio Processing Chain

| Step | Description | Configuration |
|------|-------------|---------------|
| **ğŸµ Audio Loading** | Consistent sample rate loading | 22,050 Hz |
| **âœ‚ï¸ Duration Normalization** | Trim/pad to fixed length | 3 seconds |
| **ğŸ“Š Feature Extraction** | Multi-domain feature extraction | 13 MFCCs + spectral |
| **âš–ï¸ Standardization** | Zero mean, unit variance scaling | StandardScaler |
| **ğŸ·ï¸ Label Encoding** | Numerical emotion mapping | LabelEncoder |

### Extracted Features

<details>
<summary><strong>ğŸ“ˆ Spectral Features</strong></summary>

- **MFCCs**: Mean & std of Mel-frequency cepstral coefficients
- **Chroma Features**: Pitch class information (mean & std)
- **Mel Spectrogram**: Detailed spectral representation
- **Spectral Centroid**: Brightness indicator
- **Spectral Bandwidth**: Frequency range measure
- **Spectral Rolloff**: 85% energy threshold
- **Zero Crossing Rate**: Speech/music discrimination

</details>

<details>
<summary><strong>ğŸ¼ Musical Features</strong></summary>

- **Tempo Estimation**: Rhythm analysis
- **Harmonic Components**: Tonal content
- **Percussive Components**: Transient detection

</details>

---

## ğŸ¤– Model Architectures

### ğŸ§  Deep Neural Network (DNN)
```python
Architecture: Multi-layer dense network
Regularization: Dropout layers
Optimization: Early stopping + learning rate scheduling
Performance: ~65% validation accuracy
```

### ğŸŒ² Ensemble Methods
```python
Models: XGBoost, Gradient Boosting, Enhanced Random Forest
Features: Automated feature selection
Optimization: Class weighting for imbalanced data
```

### ğŸ“Š Baseline
```python
Model: Random Forest Classifier
Purpose: Initial benchmarking and comparison
```

---

## ğŸ“ˆ Performance Metrics

### ğŸ¯ Evaluation Criteria

| Metric | Target | Status |
|--------|---------|---------|
| **Overall Accuracy** | > 80% | ğŸ”„ In Progress |
| **Weighted F1-Score** | > 80% | ğŸ”„ In Progress |
| **Per-Class Accuracy** | > 75% | ğŸ”„ In Progress |

### ğŸ† Current Results (DNN)

| Emotion | Accuracy | Performance |
|---------|----------|-------------|
| **Calm** | 88% | âœ… Excellent |
| **Angry** | 70% | âš ï¸ Good |
| **Happy** | 67% | âš ï¸ Good |
| **Others** | Varies | ğŸ”„ Improving |

> **Note**: Enhanced pipelines with feature selection and ensemble methods show improved performance across all emotion classes.

---

## ğŸš€ Quick Start

### 1. ğŸ“¦ Installation

```bash
# Clone the repository
git clone <repository-url>
cd speech-emotion-recognition

# Install dependencies
pip install -r requirements.txt
```

### 2. ğŸ“ Training

#### Option A: Deep Neural Network
```bash
# Open Jupyter notebook
jupyter notebook "complete ipynb(DNN).ipynb"
```

#### Option B: XGBoost Ensemble
```bash
# Open Jupyter notebook
jupyter notebook "complete ipynb(Xgboost).ipynb"
```

### 3. ğŸ§ª Testing

#### Single File Prediction
```bash
python test_model.py \
    --audio path/to/audio.wav \
    --model trained_models/emotion_model.h5 \
    --scaler trained_models/scaler.pkl \
    --encoder trained_models/label_encoder.pkl \
    --config trained_models/config.pkl
```

#### Batch Processing
```bash
python test_model.py --audio path/to/directory/
```

### 4. ğŸŒ Web Application

```bash
# Launch interactive web interface
streamlit run app.py
```

**Features:**
- ğŸ“¤ Audio file upload
- ğŸ“Š Real-time emotion prediction
- ğŸ“ˆ Probability visualization
- ğŸ” Feature analysis dashboard

---

## ğŸ“ Project Structure

```
speech-emotion-recognition/
â”œâ”€â”€ ğŸ““ complete ipynb(DNN).ipynb          # Deep learning training
â”œâ”€â”€ ğŸ““ complete ipynb(Xgboost).ipynb     # Ensemble methods training
â”œâ”€â”€ ğŸ test_model.py                      # CLI testing interface
â”œâ”€â”€ ğŸŒ app.py                            # Streamlit web application
â”œâ”€â”€ ğŸ“‹ requirements.txt                   # Dependencies
â”œâ”€â”€ ğŸ“ trained_models/                    # Model artifacts
â”‚   â”œâ”€â”€ emotion_model.h5                 # Trained model
â”‚   â”œâ”€â”€ scaler.pkl                       # Feature scaler
â”‚   â”œâ”€â”€ label_encoder.pkl                # Label encoder
â”‚   â””â”€â”€ config.pkl                       # Configuration
â”œâ”€â”€ ğŸ“ demo/                             # Demonstration materials
â”‚   â””â”€â”€ demo_video.mp4                   # Usage demonstration
â””â”€â”€ ğŸ“– README.md                         # This file
```

---

## ğŸ¬ Demo

ğŸ¥ **Check out our demonstration video**: `demo/demo_video.mp4`

The demo showcases:
- Real-time emotion classification
- Web interface functionality
- Model performance across different emotions
- Feature visualization capabilities

---

## ğŸ”¬ Technical References

Our methodology aligns with cutting-edge research in speech emotion recognition:

1. **"Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition"**
   - Advanced preprocessing techniques
   - CNN architecture optimization

2. **"Speech emotion classification using attention based network and regularized feature selection"**
   - Attention mechanisms
   - Feature selection strategies

---

## ğŸ¤ Contributing

We welcome contributions! Please feel free to submit pull requests, report bugs, or suggest new features.

### Development Guidelines
- Follow PEP 8 style conventions
- Add unit tests for new features
- Update documentation as needed
- Ensure backward compatibility

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- RAVDESS dataset contributors
- Open source community for amazing libraries
- Research community for advancing speech emotion recognition

---

<div align="center">

**Built with â¤ï¸ for the speech processing community**

[â­ Star this repo](../../stargazers) â€¢ [ğŸ› Report Bug](../../issues) â€¢ [ğŸ’¡ Request Feature](../../issues)

</div>




